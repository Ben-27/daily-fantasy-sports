import pandas as pd
import numpy as np

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, KFold





dat = pd.read_csv('features_lookback_10.csv')

# standardize columns
dat_std = dat[dat.loc[:, 'FP_1':'FP_10'].mean(axis='columns')>10].drop(columns=['url_suffix', 'Date']).values
scaler = StandardScaler().fit(dat_std)
dat_std = scaler.transform(dat_std, copy=True)

X = dat_std[:, 1:]
y = dat_std[:, 0]
names = dat.drop(columns=['url_suffix', 'Date', 'FP']).columns





from sklearn.linear_model import LinearRegression

kf = KFold(n_splits=10, shuffle=True, random_state=434)
reg = LinearRegression()

cv_scores = cross_val_score(reg, X, y, cv=kf)

np.quantile(cv_scores, [.05, .95])





from sklearn.linear_model import Ridge

alphas = [.1, 1, 10, 100, 1000]
ridge_scores = []

for alpha in alphas:
    ridge = Ridge(alpha=alpha)

    cv_scores = cross_val_score(ridge, X, y, cv=kf)
    print(f'alpha = {alpha} \tmean = {round(np.mean(cv_scores),3)} \tquantile = {np.quantile(cv_scores, [.5, .95])}')

    ridge_scores.append(cv_scores)    





from sklearn.linear_model import Lasso

alphas = [.1, .01, .001]
lasso_scores = []

for alpha in alphas:
    lasso = Lasso(alpha=alpha)

    cv_scores = cross_val_score(lasso, X, y, cv=kf)
    print(f'alpha = {alpha} \tmean = {round(np.mean(cv_scores),3)} \tquantile = {np.quantile(cv_scores, [.5, .95])}')

    lasso_scores.append(cv_scores)    





from xgboost import XGBRegressor

xgb_scores = []
n_param = [4, 7, 10, 13, 16, 19]

for n in n_param:
    xgb = XGBRegressor(n_estimators=n)
    
    cv_scores = cross_val_score(xgb, X, y, cv=kf)
    print(f'n_estimators = {n} \tmean = {round(np.mean(cv_scores), 3)} \tquantile = {np.quantile(cv_scores, [.05, .95])}')

    xgb_scores.append(cv_scores)
